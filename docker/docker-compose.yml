services:
  rag-app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: rag-app
    env_file:
      - .env
    environment:
      DATA_DIR: ${DATA_DIR}                         # Ordner der Dokumente
      INDEX_DIR: ${INDEX_DIR}                       # Ordner der Datenbank
      OLLAMA_HOST: "http://ollama:11434"            # Ollama Host Adresse
      OLLAMA_MODEL: ${OLLAMA_MODEL}                 # Ollama Modell
      LOG_DIR: /logs                                # Log Dateien
      EMBEDDING_MODEL_DIR: ${EMBEDDING_MODEL_DIR}   # Embedding Modelle
      HF_HOME: /root/.cache/huggingface             # Fester Ordner fuer HF Cache
      ANONYMIZED_TELEMETRY: "False"                 # ChromaDB Telemetrie deaktivieren
    ports:
      - "8000:8000"
    volumes:
      - ../src/data:/src/data
      - ../src/scripts:/src/scripts
      - ../hf-cache:/root/.cache/huggingface
      - ../logs:/logs
      - ../emb_models:/emb_models
    depends_on:
      - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  ollama:
    container_name: ollama
    image: ollama/ollama@sha256:d4188c1dfa870386a14e299976aed96daeb83876b69e1a852c9d09ea76463b9f
    env_file:
      - .env
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama           # Caching Ollama Modelle
      - ./entrypoint.sh:/entrypoint.sh
    environment:
      OLLAMA_MODEL: ${OLLAMA_MODEL}         # Ollama Modell
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]

volumes:
  ollama-data: